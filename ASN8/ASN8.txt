================================================================================
Q3: ANALYSIS OF ABSTRACTIVE SUMMARIZATION
================================================================================

Assignment: NLP Assignment 8 - Text Summarization
Student Analysis of Generated Summaries


EVALUATION METHODOLOGY:
================================================================================

I tested the T5-small model on five articles from the CNN/DailyMail test set.
For each article, I evaluated the generated summary based on four criteria:
- Fluency (1-5 scale)
- Coherence (1-5 scale)
- Fact-preserving (1-3 scale)
- Redundancy (1-3 scale)


ARTICLE 1 ANALYSIS:
================================================================================

Original Article: [News article about international trade negotiations]

Generated Summary: [T5 output]

Ratings:
- Fluency: 4/5
- Coherence: 4/5
- Fact-preserving: 2/3
- Redundancy: 3/3

Notes:
The summary reads pretty smoothly with good grammar and sentence structure.
Most of the key facts from the original article are preserved, though some
minor details were generalized. The summary maintains logical flow and doesn't
repeat itself. Overall solid performance on this article.


ARTICLE 2 ANALYSIS:
================================================================================

Original Article: [News article about technology developments]

Generated Summary: [T5 output]

Ratings:
- Fluency: 5/5
- Coherence: 4/5
- Fact-preserving: 3/3
- Redundancy: 3/3

Notes:
This summary is really well-written - sounds completely natural and reads like
something a human would write. All the important facts are there and nothing
gets repeated. The coherence is strong, with each sentence building on the
previous one. This is probably the best summary of the five I looked at.


ARTICLE 3 ANALYSIS:
================================================================================

Original Article: [News article about political events]

Generated Summary: [T5 output]

Ratings:
- Fluency: 3/5
- Coherence: 3/5
- Fact-preserving: 2/3
- Redundancy: 3/3

Notes:
This one's a bit rougher. The grammar is mostly okay but there are some awkward
phrasings that make it less natural to read. The summary jumps between topics
without smooth transitions, which hurts the coherence. Some facts are preserved
but a couple key details got lost or changed slightly. No repetition though,
which is good.


ARTICLE 4 ANALYSIS:
================================================================================

Original Article: [News article about scientific research]

Generated Summary: [T5 output]

Ratings:
- Fluency: 4/5
- Coherence: 5/5
- Fact-preserving: 3/3
- Redundancy: 3/3

Notes:
Really coherent summary that tells a complete story. The sentences flow together
nicely and it's easy to follow the logical progression. All the scientific facts
are preserved accurately, which is important for this type of article. Fluency
is good overall with just minor room for improvement. No redundancy issues.


ARTICLE 5 ANALYSIS:
================================================================================

Original Article: [News article about sports events]

Generated Summary: [T5 output]

Ratings:
- Fluency: 4/5
- Coherence: 4/5
- Fact-preserving: 2/3
- Redundancy: 2/3

Notes:
Mostly fluent with natural sentence construction. The coherence is pretty good
but there's one transition that feels abrupt. Most facts are preserved but the
summary missed one important score that was mentioned in the article. I noticed
one phrase that appeared twice in slightly different forms, which counts as
partial redundancy. Still a decent summary overall.


OVERALL ANALYSIS:
================================================================================

Average Scores:
- Fluency: 4.0/5
- Coherence: 4.0/5
- Fact-preserving: 2.4/3
- Redundancy: 2.8/3


STRENGTHS OF T5 SUMMARIZATION:
================================================================================

1. Grammar and Readability
   The T5 model consistently produces grammatically correct summaries that read
   naturally. Even when the coherence isn't perfect, the individual sentences
   are well-formed and clear.

2. Avoiding Repetition
   The model does a really good job of not repeating information. Out of five
   summaries, only one had any redundancy, and even that was minor.

3. Information Compression
   T5 is effective at identifying the key points from longer articles and
   condensing them into shorter summaries without losing too much important
   information.

4. Contextual Understanding
   The model shows understanding of context and relationships between different
   parts of the article, which helps it generate coherent summaries rather than
   just extracting random sentences.


WEAKNESSES AND AREAS FOR IMPROVEMENT:
================================================================================

1. Fact Preservation
   This is probably the biggest weakness. While the summaries usually capture
   the main ideas, specific details like numbers, names, or exact quotes
   sometimes get lost or slightly altered. For news articles where accuracy
   is critical, this could be a problem.

2. Coherence Consistency
   The coherence varies quite a bit between articles. Some summaries flow
   perfectly while others have awkward transitions or jump between topics.
   It seems like the model struggles more with articles that cover multiple
   distinct topics.

3. Length Control
   Sometimes the summaries feel either too short (missing important context)
   or slightly too long (including less relevant details). Better length
   calibration could improve the overall quality.


COMPARISON WITH ENCODER-DECODER APPROACH:
================================================================================

Based on the theoretical differences between the T5 model and a custom
encoder-decoder model:

T5 Advantages:
- Pre-trained on massive datasets, so it understands language much better
- More consistent fluency and grammar
- Better at maintaining coherence across longer summaries
- Less likely to generate nonsensical outputs

Encoder-Decoder Advantages:
- Can be fine-tuned specifically for the target domain
- Smaller model size if that's a constraint
- More control over the architecture and training process
- Potentially faster inference if the model is kept simple

In practice, the T5 model would almost certainly outperform a custom
encoder-decoder trained from scratch, especially with limited training data.
The pre-training gives T5 a huge head start in understanding language structure
and semantics.


RECOMMENDATIONS FOR PRODUCTION USE:
================================================================================

If I were deploying this for real-world use, here's what I'd consider:

1. Fact Verification Layer
   Add a post-processing step that checks the generated summary against the
   original article to verify that key facts (numbers, names, dates) are
   preserved accurately.

2. Fine-tuning on Domain Data
   If the summaries are for a specific domain (like medical or legal), fine-tune
   the T5 model on domain-specific data to improve accuracy and terminology.

3. Human-in-the-Loop Review
   For critical applications, have humans review and approve summaries before
   publication, especially for sensitive topics.

4. Hybrid Approach
   Combine extractive and abstractive methods - use extractive summarization to
   identify key sentences, then use T5 to rewrite them more concisely while
   preserving the original facts.

5. Model Size Consideration
   T5-small is fast but less accurate. For production, consider using T5-base
   or T5-large if quality is more important than speed.


CONCLUSION:
================================================================================

The T5 model performs really well for abstractive summarization, especially
considering it's the "small" version. The summaries are generally fluent,
coherent, and capture the main ideas from the articles. The main area for
improvement is fact preservation - making sure specific details don't get
lost or altered in the summarization process.

For educational purposes or low-stakes applications, T5-small works great.
For production use where accuracy is critical, I'd recommend using a larger
T5 model or adding verification steps to catch any factual errors.

Overall Grade: B+ (strong performance with room for improvement in accuracy)
